{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0BEIoEWpNXJj",
        "outputId": "850506e4-bdb2-4d06-db8c-4edc72107aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/archive.zip, /content/archive.zip.zip or /content/archive.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "SyMcRitZODeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOwKG3pCyoLR",
        "outputId": "40bb57ad-d5e9-40e1-dbb9-894aa66f0431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "archive.zip\t\t\t\t\t consolidated  test\n",
            "autism-2.txt\t\t\t\t\t drive\t       train\n",
            "autism-S-224-89.33.h5\t\t\t\t kaggle.json   valid\n",
            "autistic-children-facial-data-set-metadata.json  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "y7U4lHHczSbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
       import os                           # For filesystem operations
from PIL import Image               # For loading images
import torch                        # Core PyTorch
import torch.nn as nn               # Neural network modules
import torch.optim as optim         # Optimizers
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms   # Image transforms
from torchvision.models import resnet18, ResNet18_Weights  # Pretrained ResNet-18
from timm.models.vision_transformer import vit_base_patch16_224  # Pretrained ViT

# -------------------------------------------------------
# Device configuration
# -------------------------------------------------------
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# -------------------------------------------------------
# Hyperparameters and constants
# -------------------------------------------------------
num_classes = 7          # Number of emotion categories
batch_size = 32          # Batch size
learning_rate = 1e-4     # Base learning rate for fine-tuning
num_epochs = 25          # Maximum number of epochs (with early stopping)
image_size = 224         # Input resolution for ResNet and ViT

# -------------------------------------------------------
# Image transforms (augmentation + normalization)
# -------------------------------------------------------
transform = transforms.Compose([
    transforms.Resize((image_size, image_size)),       # Standardize spatial size
    transforms.RandomHorizontalFlip(),                 # Random horizontal flip
    transforms.RandomRotation(15),                     # Small random rotation
    transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),        # Color augmentation
    transforms.ToTensor(),                             # PIL -> tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # ImageNet normalization
                         std=[0.229, 0.224, 0.225])
])

# -------------------------------------------------------
# Custom Dataset for folder-structured images
# root_dir/
#    class_0/
#    class_1/
#    ...
# -------------------------------------------------------
class AutismFacialDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = []

        # Discover class folders
        self.classes = sorted(
            entry.name for entry in os.scandir(root_dir) if entry.is_dir()
        )
        self.class_to_idx = {
            cls_name: idx for idx, cls_name in enumerate(self.classes)
        }

        # Collect (image_path, label) pairs
        for cls_name in self.classes:
            cls_dir = os.path.join(root_dir, cls_name)
            for fname in os.listdir(cls_dir):
                fpath = os.path.join(cls_dir, fname)
                if os.path.isfile(fpath) and fname.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.samples.append((fpath, self.class_to_idx[cls_name]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image, label

# -------------------------------------------------------
# Dataset loading and splitting (70/15/15)
# -------------------------------------------------------
dataset_path = './consolidated'   # Root folder of your dataset
full_dataset = AutismFacialDataset(root_dir=dataset_path, transform=transform)

total_size = len(full_dataset)
train_size = int(0.7 * total_size)
val_size = int(0.15 * total_size)
test_size = total_size - train_size - val_size  # ensures full coverage

train_dataset, val_dataset, test_dataset = random_split(
    full_dataset,
    [train_size, val_size, test_size],
    generator=torch.Generator().manual_seed(42)  # reproducible split
)

train_loader = DataLoader(train_dataset, batch_size=batch_size,
                          shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=batch_size,
                        shuffle=False, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=batch_size,
                         shuffle=False, num_workers=4)

# -------------------------------------------------------
# Hybrid CNNâ€“ViT model definition
# -------------------------------------------------------
class FER_ViT(nn.Module):
    def __init__(self, num_classes=num_classes):
        super(FER_ViT, self).__init__()

        # Local feature extractor: ResNet-18 pretrained on ImageNet
        self.backbone = resnet18(weights=ResNet18_Weights.DEFAULT)
        self.backbone.fc = nn.Identity()  # remove final FC, keep 512-dim features

        # Global feature extractor: Vision Transformer (ViT-B/16, 224x224)
        self.vit = vit_base_patch16_224(pretrained=True)

        # Freeze ResNet except last residual block (layer4) for fine-tuning
        for name, param in self.backbone.named_parameters():
            param.requires_grad = False
            if 'layer4' in name:
                param.requires_grad = True

        # Make all ViT parameters trainable
        for param in self.vit.parameters():
            param.requires_grad = True

        # Fusion layer: concatenate 512-d ResNet + 768-d ViT class token -> 512-d
        self.feature_fusion = nn.Sequential(
            nn.Linear(512 + 768, 512),
            nn.BatchNorm1d(512),
            nn.ReLU()
        )

        # Classifier head: dropout + linear to num_classes
        self.classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        # Local features from ResNet (batch, 512)
        local_features = self.backbone(x)

        # Global features from ViT; forward_features returns tokens
        vit_features = self.vit.forward_features(x)
        # Use the [CLS] token (first token) as global representation (batch, 768)
        vit_class_token = vit_features[:, 0]

        # Concatenate local and global features
        combined_features = torch.cat((local_features, vit_class_token), dim=1)

        # Fuse and classify
        fused = self.feature_fusion(combined_features)
        out = self.classifier(fused)
        return out

# -------------------------------------------------------
# Instantiate model, loss, optimizer, scheduler
# -------------------------------------------------------
model = FER_ViT().to(device)

criterion = nn.CrossEntropyLoss()  # can add class weights if needed

# Only update parameters that require gradients (unfrozen)
optimizer = optim.Adam(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=learning_rate
)

# CosineAnnealingLR for smooth learning rate decay
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# -------------------------------------------------------
# Training, validation, and test functions
# -------------------------------------------------------
def train():
    """One epoch of training on train_loader."""
    model.train()
    total_loss, correct, total = 0.0, 0, 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    avg_loss = total_loss / total
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy


def validate():
    """Evaluate on val_loader without gradient updates."""
    model.eval()
    total_loss, correct, total = 0.0, 0, 0

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    avg_loss = total_loss / total
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy


def test():
    """Final evaluation on held-out test_loader."""
    model.eval()
    total_loss, correct, total = 0.0, 0, 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    avg_loss = total_loss / total
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy

# -------------------------------------------------------
# Training loop with early stopping (on validation loss)
# -------------------------------------------------------
patience = 10                  # stop if no val-loss improvement for this many epochs
best_val_loss = float('inf')  # best (lowest) validation loss seen so far
best_val_acc = 0.0            # validation accuracy at best loss
best_epoch = -1
epochs_no_improve = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train()
    val_loss, val_acc = validate()

    # Step scheduler (use val_loss if you switch to ReduceLROnPlateau)
    scheduler.step()

    # Early stopping check on validation loss (with small delta to avoid noise)
    if val_loss < best_val_loss - 1e-4:
        best_val_loss = val_loss
        best_val_acc = val_acc
        best_epoch = epoch + 1
        epochs_no_improve = 0
        torch.save(model.state_dict(), 'fer_vit_autism_best.pth')
    else:
        epochs_no_improve += 1

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% "
          f"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}% "
          f"(no improve: {epochs_no_improve})")

    if epochs_no_improve >= patience:
        print(f"\nEarly stopping triggered at epoch {epoch+1}")
        break

print(f"\nBest validation loss: {best_val_loss:.4f} "
      f"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}")

# -------------------------------------------------------
# Final test evaluation using best model
# -------------------------------------------------------
model.load_state_dict(torch.load('fer_vit_autism_best.pth'))
test_loss, test_acc = test()
print(f"Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}%")
