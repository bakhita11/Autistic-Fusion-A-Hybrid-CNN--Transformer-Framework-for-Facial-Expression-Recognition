{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0BEIoEWpNXJj",
        "outputId": "850506e4-bdb2-4d06-db8c-4edc72107aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/archive.zip, /content/archive.zip.zip or /content/archive.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "SyMcRitZODeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOwKG3pCyoLR",
        "outputId": "40bb57ad-d5e9-40e1-dbb9-894aa66f0431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "archive.zip\t\t\t\t\t consolidated  test\n",
            "autism-2.txt\t\t\t\t\t drive\t       train\n",
            "autism-S-224-89.33.h5\t\t\t\t kaggle.json   valid\n",
            "autistic-children-facial-data-set-metadata.json  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "y7U4lHHczSbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # For directory and file path manipulations\n",
        "from PIL import Image  # For image loading and processing\n",
        "import torch  # Main PyTorch library for tensor operations\n",
        "import torch.nn as nn  # Neural network related modules\n",
        "import torch.optim as optim  # Optimization algorithms like Adam\n",
        "from torch.utils.data import Dataset, DataLoader, random_split  # Dataset and utilities for batching and splitting\n",
        "import torchvision.transforms as transforms  # Image augmentation and preprocessing\n",
        "from torchvision.models import resnet18  # Pretrained ResNet-18 model\n",
        "from timm.models.vision_transformer import vit_base_patch16_224  # Pretrained Vision Transformer model\n",
        "\n",
        "# Device configuration: choose GPU if available else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Hyperparameters and constants\n",
        "num_classes = 7  # Number of emotion classes in dataset\n",
        "batch_size = 32  # Mini-batch size for training/validation\n",
        "learning_rate = 3e-4  # Initial learning rate for optimizer\n",
        "num_epochs = 30  # Number of training epochs\n",
        "image_size = 224  # Image size input to ViT and ResNet models\n",
        "\n",
        "\n",
        "# Compose image preprocessing transformations: resize, convert to tensor, normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),  # Resize input images to 224x224\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize using ImageNet means\n",
        "                         std=[0.229, 0.224, 0.225])   # Normalize using ImageNet standard deviations\n",
        "])\n",
        "\n",
        "\n",
        "# Custom PyTorch Dataset class to handle dataset organized by class subfolders\n",
        "class AutismFacialDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize dataset by scanning images under root_dir organized in class folders.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Directory containing class-specific subfolders of images.\n",
        "            transform (callable, optional): Image transformations to apply.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir  # Root dataset directory path\n",
        "        self.transform = transform  # Transform pipeline for images\n",
        "        self.samples = []  # List to hold tuples (image_path, label)\n",
        "        self.classes = []  # List of class folder names (e.g., 'ASD', 'Non-ASD')\n",
        "        self.class_to_idx = {}  # Mapping from class name to numerical label\n",
        "\n",
        "        # Discover class folders sorted alphabetically\n",
        "        self.classes = sorted(entry.name for entry in os.scandir(root_dir) if entry.is_dir())\n",
        "        # Create dict mapping class name to integer label\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        # Walk through each class folder to collect image file paths and assign labels\n",
        "        for cls_name in self.classes:\n",
        "            cls_dir = os.path.join(root_dir, cls_name)  # Full path to class folder\n",
        "            for fname in os.listdir(cls_dir):  # Iterate over files in class folder\n",
        "                fpath = os.path.join(cls_dir, fname)\n",
        "                # Make sure file is image with allowed extensions\n",
        "                if os.path.isfile(fpath) and fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.samples.append((fpath, self.class_to_idx[cls_name]))  # Append path and label\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of image samples\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image path and label at index idx\n",
        "        img_path, label = self.samples[idx]\n",
        "        # Open image and convert to RGB\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        # Return image tensor and corresponding label\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Root directory where dataset is extracted and arranged in class folders\n",
        "dataset_path = './data'  # Change if needed according to the folder structure\n",
        "\n",
        "# Instantiate custom dataset with images and transformations applied\n",
        "full_dataset = AutismFacialDataset(root_dir=dataset_path, transform=transform)\n",
        "\n",
        "# Calculate the dataset sizes for train, validation, and test splits\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size  # To account for rounding\n",
        "\n",
        "# Split the dataset randomly into train, validation, and test subsets\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # Ensures reproducibility\n",
        ")\n",
        "\n",
        "# Create DataLoader objects for each subset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Define the facial expression recognition model combining ResNet and ViT backbones\n",
        "class FER_ViT(nn.Module):\n",
        "    def __init__(self, num_classes=num_classes):\n",
        "        super(FER_ViT, self).__init__()\n",
        "        # Load pretrained ResNet-18 as local feature extractor\n",
        "        self.backbone = resnet18(pretrained=True)\n",
        "        self.backbone.fc = nn.Identity()  # Remove final classification layer\n",
        "\n",
        "        # Load pretrained Vision Transformer as global feature extractor\n",
        "        self.vit = vit_base_patch16_224(pretrained=True)\n",
        "\n",
        "        # Freeze ResNet parameters to prevent updating during training\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Allow ViT parameters to be learned/fine-tuned\n",
        "        for param in self.vit.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Fully connected layer to fuse ResNet (512) and ViT (768) features to 512 features\n",
        "        self.feature_fusion = nn.Linear(512 + 768, 512)\n",
        "\n",
        "        # Final classifier layers with non-linearity and dropout regularization\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract local features using ResNet backbone\n",
        "        local_features = self.backbone(x)\n",
        "        # Extract global features using ViT backbone\n",
        "        vit_features = self.vit.forward_features(x)\n",
        "        # Concatenate features from both backbones\n",
        "        combined_features = torch.cat((local_features, vit_features), dim=1)\n",
        "        # Fuse concatenated features to compact vector\n",
        "        fused = self.feature_fusion(combined_features)\n",
        "        # Pass fused features through classifier to get final logits\n",
        "        out = self.classifier(fused)\n",
        "        return out\n",
        "\n",
        "# Instantiate model and send to computation device (GPU/CPU)\n",
        "model = FER_ViT().to(device)\n",
        "\n",
        "\n",
        "# Define loss function as Cross-Entropy (appropriate for multi-class classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define Adam optimizer with configured learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# Learning rate scheduler to decay LR by half every 10 epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training function for one epoch\n",
        "def train():\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to device\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        optimizer.zero_grad()  # Reset previous gradients\n",
        "        loss.backward()  # Backpropagation to compute gradients\n",
        "        optimizer.step()  # Update model weights\n",
        "        total_loss += loss.item() * images.size(0)  # Accumulate loss weighted by batch size\n",
        "        _, predicted = outputs.max(1)  # Get predictions from logits\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()  # Count correct predictions\n",
        "    avg_loss = total_loss / total  # Average loss per sample\n",
        "    accuracy = 100.0 * correct / total  # Accuracy percentage\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "# Validation function to evaluate model performance\n",
        "def validate():\n",
        "    model.eval()  # Set model to evaluation mode (affects dropout, batchnorm)\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Main training loop over epochs\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train()  # Train for one epoch\n",
        "    val_loss, val_acc = validate()   # Validate on validation set\n",
        "    scheduler.step()  # Step learning rate scheduler\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"  # Print epoch stats\n",
        "          f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% \"\n",
        "          f\"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# Save the trained model state dictionary to disk\n",
        "torch.save(model.state_dict(), 'fer_vit_autism.pth')\n"
      ],
      "metadata": {
        "id": "4YPZLC0i8Zpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}