# ------------------------------- Imports -------------------------------
import os                               # Provides tools for interacting with the operating system (folders/files)
from PIL import Image                   # Imports PIL Image for opening and converting image files
import torch                            # Imports PyTorch core library for tensors and GPU support
import torch.nn as nn                   # Imports neural network layers/modules (e.g., Linear, ReLU, Dropout)
import torch.optim as optim             # Imports optimizers (e.g., Adam) and learning-rate schedulers
from torch.utils.data import Dataset, DataLoader, random_split  # Dataset base class, DataLoader, and dataset splitting utility
import torchvision.transforms as transforms   # Common image preprocessing and augmentation transforms
from torchvision.models import resnet18, ResNet18_Weights        # Pretrained ResNet-18 model and its weight enums
from timm.models.vision_transformer import vit_base_patch16_224  # Pretrained Vision Transformer (ViT-B/16) from timm

# ------------------------------- Device configuration -------------------------------
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, otherwise CPU

# ------------------------------- Hyperparameters and constants -------------------------------
num_classes = 4          # Number of emotion categories (Updated to 4 based on available folders)
batch_size = 32          # Number of samples per mini-batch
learning_rate = 5e-5     # Learning rate (small for stable fine-tuning, especially for ViT)
num_epochs = 25          # Maximum number of training epochs (early stopping can stop sooner)
image_size = 224         # Input image size required by ResNet-18 and ViT-B/16

# ------------------------------- Class order (explicit mapping) -------------------------------
FERAC_CLASSES = ["anger", "fear", "joy", "Natural"]  # Fixed label order matching actual folder names

# ------------------------------- Image transforms -------------------------------
transform = transforms.Compose([                                     # Chain multiple transforms into one pipeline
    transforms.Resize((image_size, image_size)),                     # Resize images to 224x224 for consistent input size
    transforms.RandomHorizontalFlip(p=0.5),                          # Randomly flip image horizontally (50% chance)
    transforms.RandomRotation(10),                                   # Randomly rotate image up to +/- 10 degrees
    transforms.RandomApply(                                          # Apply the following transform(s) only with probability p
        [transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)],                # Randomly change brightness/contrast/saturation/hue
        p=0.7                                                        # Apply ColorJitter with 70% probability
    ),
    transforms.RandomGrayscale(p=0.1),                               # Convert image to grayscale with 10% probability
    transforms.ToTensor(),                                           # Convert PIL image to PyTorch tensor (C x H x W)
    transforms.Normalize(mean=[0.485, 0.456, 0.406],                 # Normalize using ImageNet mean (per channel)
                         std=[0.229, 0.224, 0.225])                  # Normalize using ImageNet std (per channel)
])

# ------------------------------- Custom Dataset class -------------------------------
class FERACDataset(Dataset):                                         # Define a custom dataset class inheriting from torch Dataset
    def __init__(self, root_dir, transform=None):                    # Constructor: takes dataset root folder and transform pipeline
        self.root_dir = root_dir                                     # Store root directory path (e.g., "./consolidated")
        self.transform = transform                                   # Store transforms to apply to each loaded image
        self.samples = []                                            # Initialize empty list to store (filepath, label) pairs

        self.classes = FERAC_CLASSES                                 # Use fixed class list to enforce consistent labels
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}  # Map class name -> numeric index

        # Iterate through 'train' and 'test' subdirectories
        for subset_name in ['train', 'test']:
            subset_dir = os.path.join(root_dir, subset_name)
            if not os.path.isdir(subset_dir):
                print(f"Warning: Missing subset folder: {subset_dir}. Skipping.")
                continue

            for cls_name in self.classes:                                # Loop through each class folder (angry, disgust, ...)
                cls_dir = os.path.join(subset_dir, cls_name)               # Build full folder path for the class
                if not os.path.isdir(cls_dir):
                    raise FileNotFoundError(f"Missing class folder: {cls_dir}")

                for fname in os.listdir(cls_dir):
                    fpath = os.path.join(cls_dir, fname)
                    if os.path.isfile(fpath) and fname.lower().endswith(('.png', '.jpg', '.jpeg')):
                        self.samples.append((fpath, self.class_to_idx[cls_name]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image, label

# ------------------------------- Dataset loading and splitting -------------------------------
dataset_path = './FERAC Dataset'                                     # Root folder containing FERAC class subfolders
full_dataset = FERACDataset(root_dir=dataset_path, transform=transform)  # Create dataset instance with transforms

total_size = len(full_dataset)                                       # Total number of images in dataset
train_size = int(0.7 * total_size)                                   # Compute training set size (70%)
val_size = int(0.15 * total_size)                                    # Compute validation set size (15%)
test_size = total_size - train_size - val_size                       # Assign remaining images to test set (ensures exact total)

train_dataset, val_dataset, test_dataset = random_split(             # Split full dataset into train/val/test subsets
    full_dataset,
    [train_size, val_size, test_size],
    generator=torch.Generator().manual_seed(42)
)

train_loader = DataLoader(train_dataset, batch_size=batch_size,
                          shuffle=True, num_workers=4,
                          pin_memory=True, persistent_workers=True)

val_loader = DataLoader(val_dataset, batch_size=batch_size,
                        shuffle=False, num_workers=4,
                        pin_memory=True, persistent_workers=True)

test_loader = DataLoader(test_dataset, batch_size=batch_size,
                         shuffle=False, num_workers=4,
                         pin_memory=True, persistent_workers=True)

# ------------------------------- Hybrid CNNâ€“ViT model definition -------------------------------
class FER_ViT(nn.Module):
    def __init__(self, num_classes=num_classes):
        super(FER_ViT, self).__init__()

        self.backbone = resnet18(weights=ResNet18_Weights.DEFAULT)
        self.backbone.fc = nn.Identity()

        self.vit = vit_base_patch16_224(pretrained=True)

        for name, param in self.backbone.named_parameters():
            param.requires_grad = False
            if 'layer3' in name or 'layer4' in name:
                param.requires_grad = True

        for param in self.vit.parameters():
            param.requires_grad = True

        self.feature_fusion = nn.Sequential(
            nn.Linear(512 + 768, 512),
            nn.BatchNorm1d(512),
            nn.ReLU()
        )

        self.classifier = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        local_features = self.backbone(x)

        vit_out = self.vit.forward_features(x)
        if vit_out.ndim == 3:
            vit_class_token = vit_out[:, 0, :]
        else:
            vit_class_token = vit_out

        combined_features = torch.cat((local_features, vit_class_token), dim=1)
        fused = self.feature_fusion(combined_features)
        out = self.classifier(fused)
        return out

# ------------------------------- Instantiate model, loss, optimizer, scheduler -------------------------------
model = FER_ViT().to(device)

criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=learning_rate
)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=2
)

# ------------------------------- Training, validation, and test functions -------------------------------
def train():
    model.train()
    total_loss, correct, total = 0.0, 0, 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * images.size(0)
        predicted = outputs.argmax(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    avg_loss = total_loss / total
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy


def validate():
    model.eval()
    total_loss, correct, total = 0.0, 0, 0

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * images.size(0)
            predicted = outputs.argmax(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    avg_loss = total_loss / total
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy


def test():
    model.eval()
    total_loss, correct, total = 0.0, 0, 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * images.size(0)
            predicted = outputs.argmax(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    avg_loss = total_loss / total
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy

# ------------------------------- Training loop with early stopping -------------------------------
patience = 10
best_val_loss = float('inf')
best_val_acc = 0.0
best_epoch = -1
epochs_no_improve = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train()
    val_loss, val_acc = validate()

    scheduler.step(val_loss)

    if val_loss < best_val_loss - 1e-4:
        best_val_loss = val_loss
        best_val_acc = val_acc
        best_epoch = epoch + 1
        epochs_no_improve = 0
        torch.save(model.state_dict(), 'fer_vit_best.pth')
    else:
        epochs_no_improve += 1

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% "
          f"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}% "
          f"(no improve: {epochs_no_improve})")

    if epochs_no_improve >= patience:
        print(f"\nEarly stopping triggered at epoch {epoch+1}")
        break

print(f"\nBest validation loss: {best_val_loss:.4f} "
      f"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}")

# ------------------------------- Final test evaluation using best model -------------------------------
model.load_state_dict(torch.load('fer_vit_best.pth', map_location=device))
test_loss, test_acc = test()
print(f"Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}%")
